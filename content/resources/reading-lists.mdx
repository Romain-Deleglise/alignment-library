---
title: "Reading Lists par Niveau"
description: "Ressources organisées par niveau de difficulté"
difficulty: "Beginner"
---

# Reading Lists par Niveau

## Débutant (0-20h)

**Objectif**: Comprendre qu'il y a un problème

### 1. AGI Ruin: A List of Lethalities (Eliezer)
- https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities
- 2h lecture

### 2. AI Alignment: Why It's Hard, and Where to Start (Video - Eliezer)
- https://www.youtube.com/watch?v=EUjc1WuyPT8
- 1h

### 3. Rob Miles YouTube Channel (vulgarisation)
- https://www.youtube.com/@RobertMilesAI
- 5-10 videos (5h)

### 4. The Alignment Problem (Brian Christian - livre)
- Accessible, journalistique
- 10-15h

---

## Intermédiaire (20-100h)

**Objectif**: Comprendre problèmes techniques principaux

### 1. Risks from Learned Optimization (Hubinger et al.)
- https://arxiv.org/abs/1906.01820
- 10h (paper + discussions)

### 2. Concrete Problems in AI Safety (Amodei et al.)
- https://arxiv.org/abs/1606.06565
- 5h

### 3. Embedded Agency (Sequence)
- https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh
- 20h

### 4. Superintelligence (Nick Bostrom - livre)
- Classique, un peu daté (2014) mais fondamental
- 20h

### 5. ELK Document (ARC)
- https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8
- 10h

### 6. Alignment Forum (curated posts)
- https://www.alignmentforum.org/
- 20-30h (sélection)

---

## Avancé (100-500h)

**Objectif**: Comprendre research frontiers, contribuer

### 1. MIRI Research
- https://intelligence.org/research/
- Agent foundations papers
- 50-100h

### 2. Corrigibility (MIRI)
- https://intelligence.org/files/Corrigibility.pdf
- 20h (+ related work)

### 3. Logical Induction (MIRI)
- https://intelligence.org/files/LogicalInduction.pdf
- 40h (très math-heavy)

### 4. Iterated Amplification (Paul Christiano, all posts)
- https://ai-alignment.com/
- 30h

### 5. Constitutional AI + Mechanistic Interpretability (Anthropic research)
- https://www.anthropic.com/research
- 40h

### 6. Debate, ELK, related (ARC, OpenAI)
- Papers + discussions
- 50h

### 7. Alignment Forum (comprehensive reading)
- Major sequences, debates
- 100-200h
