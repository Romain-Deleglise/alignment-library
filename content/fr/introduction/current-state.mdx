---
title: "État actuel (2024)"
description: "État des lieux de la recherche en alignement de l'IA en 2024"
difficulty: "beginner"
readingTime: "12 min"
tags: ["fundamentals","current-ai","progress"]
prerequisites: ["introduction/what-is-alignment"]
---

# État actuel (2024)

## Ce qu'on a résolu: Presque rien

- RLHF: Superficiel, facilement contournable
- Constitutional AI: Mieux que rien, insuffisant
- Interpretability: Progrès mais pas scaling
- Formal verification: Théorique seulement

## Ce qu'on n'a PAS résolu (critiques)

- Inner alignment (mesa-optimization)
- Deceptive alignment (détection)
- Corrigibility (peut-être impossible)
- Scalable oversight (superviser superintelligence)
- Value specification (définir nos vraies valeurs)

## P(doom) estimés (chercheurs)

- Eliezer Yudkowsky: ~99%
- Paul Christiano: ~50-70%
- Nate Soares (MIRI): ~90%+
- Médiane communauté: ~60-80%

## Ressources

- [2023 AI Alignment Research Overview](https://www.alignmentforum.org/) - Alignment Forum
- [AI Safety State of the Field Report](https://www.safe.ai/state-of-ai-safety-report-2024)
