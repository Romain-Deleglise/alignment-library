---
title: "Chercheurs Clés en AI Alignment"
description: "Les chercheurs les plus influents dans le domaine de l'alignement de l'IA"
difficulty: "Beginner"
---

# Chercheurs Clés

## Eliezer Yudkowsky

**Organisation**: MIRI (co-fondateur)

**Contributions**:
- Sequences (rationalité, AI risk)
- CEV (Coherent Extrapolated Volition)
- Intelligence Explosion
- AGI Ruin: A List of Lethalities

**P(doom)**: ~99%

**Style**: Conceptual, big picture, pessimiste

**Ressources**: https://www.lesswrong.com/users/eliezer_yudkowsky

---

## Paul Christiano

**Organisation**: ARC (fondateur), ex-OpenAI

**Contributions**:
- Iterated Amplification
- Debate
- ELK
- RLHF (contributions)

**P(doom)**: ~50-70%

**Style**: Technique, pragmatique, solutions-oriented

**Ressources**: https://ai-alignment.com/

---

## Nate Soares

**Organisation**: MIRI (directeur exécutif)

**Contributions**:
- Agent foundations
- Corrigibility research
- Value learning

**P(doom)**: ~90-95%

**Ressources**: https://www.lesswrong.com/users/so8res

---

## Evan Hubinger

**Organisation**: Anthropic

**Contributions**:
- Risks from Learned Optimization (paper clé)
- Mesa-optimization, deceptive alignment (concepts)
- Sleeper Agents

**Ressources**: https://www.alignmentforum.org/users/evhub

---

## Stuart Russell

**Organisation**: UC Berkeley

**Contributions**:
- Human Compatible (livre)
- Inverse Reinforcement Learning
- CIRL (Cooperative IRL)

**P(doom)**: ~50% (publiquement)

**Style**: Académique, accessible
