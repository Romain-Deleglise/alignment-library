---
title: "Mechanistic Interpretability"
description: "Comprendre le fonctionnement interne des réseaux de neurones"
difficulty: "Advanced"
---

# Mechanistic Interpretability

## Principe

Comprendre le fonctionnement interne des neural networks (reverse-engineer).

## Approche

- Identifier circuits (sous-réseaux qui accomplissent fonctions spécifiques)
- Comprendre représentations internes
- Détecter features/behaviors indésirables

## Progrès (Anthropic, DeepMind)

- Circuits identifiés dans models simples (GPT-2)
- Quelques features interprétables (détecteurs d'objets, etc.)

## Limitations

1. **Scalability**: Techniques actuelles marchent sur petits models, pas GPT-4+
2. **Completeness**: Même si on comprend circuits, pas guarantee d'avoir tout trouvé
3. **Deceptive alignment**: IA peut cacher intentions dans représentations opaques
4. **Sufficiency**: Comprendre ≠ Contrôler

## Conclusion

Utile. Nécessaire probablement. Mais insuffisant seul.

## Ressources

- [Transformer Circuits Thread](https://transformer-circuits.pub/) - Anthropic
- [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
- [Mechanistic Interpretability](https://www.alignmentforum.org/tag/mechanistic-interpretability)
