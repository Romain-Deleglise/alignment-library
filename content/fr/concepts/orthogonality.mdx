---
title: "Thèse d'Orthogonalité"
description: "L'intelligence et les objectifs sont indépendants : tout niveau d'intelligence peut poursuivre n'importe quel objectif"
difficulty: "beginner"
readingTime: "8 min"
tags: ["fundamentals", "philosophy", "motivation"]
prerequisites: []
---

# Thèse d'Orthogonalité

## Définition

La Thèse d'Orthogonalité stipule que **l'intelligence et les objectifs finaux sont orthogonaux** (indépendants) : en principe, n'importe quel niveau d'intelligence peut être combiné avec n'importe quel objectif final.

Cela signifie qu'un système arbitrairement intelligent peut avoir des objectifs arbitrairement "stupides" ou nuisibles du point de vue humain.

## Pourquoi c'est Important

La thèse d'orthogonalité réfute une idée fausse courante : **"Une IA suffisamment intelligente convergera naturellement vers les valeurs humaines."**

Cette croyance optimiste est fausse. Il n'y a pas de connexion automatique entre :
- La capacité d'un système à atteindre des objectifs (intelligence)
- Les objectifs qu'il poursuit réellement (valeurs/objectifs)

## Contexte Historique

Proposée par Nick Bostrom dans "Superintelligence" (2014), bien que l'idée centrale ait été reconnue plus tôt dans les discussions sur la sécurité de l'IA.

## Implications Clés

### 1. Intelligence ≠ Sagesse

Une IA superintelligente optimisant pour les trombones serait incroyablement habile à fabriquer des trombones, sans aucune "sagesse" qui la ferait se soucier des humains.

### 2. Pas de Sécurité Automatique

Nous ne pouvons pas compter sur l'intelligence seule pour rendre une IA sûre. Un système peut être :
- Incroyablement intelligent en planification et raisonnement
- Complètement indifférent au bien-être humain
- Très efficace pour atteindre des objectifs que nous trouvons répréhensibles

### 3. L'Alignement est Essentiel

Parce que l'intelligence ne produit pas automatiquement des objectifs bénéfiques, nous devons explicitement aligner les systèmes d'IA avec les valeurs humaines.

## Objections Courantes

### "Mais une IA intelligente ne réaliserait-elle pas que nuire aux humains est mal ?"

Non. La moralité n'est pas une vérité logique qui peut être dérivée du raisonnement pur. Une IA doit *se soucier* du bien-être humain pour le prendre en compte dans ses décisions.

### "Sûrement qu'un système intelligent adopterait de meilleurs objectifs ?"

Cela confond :
- **Objectifs instrumentaux** (sous-objectifs utiles pour atteindre l'objectif final)
- **Objectifs finaux** (valeurs terminales que le système optimise)

Une IA pourrait adopter de meilleurs objectifs *instrumentaux* pour atteindre son objectif final plus efficacement, mais elle n'a aucune raison de changer son objectif final lui-même.

### "L'évolution nous a rendus intelligents et nous a donné des valeurs, cela ne se produira-t-il pas avec l'IA ?"

Les humains sont un cas spécial. L'objectif de l'évolution était la fitness génétique inclusive, mais :
- Nous avons développé l'intelligence générale comme stratégie instrumentale
- Cette intelligence nous a finalement amenés à poursuivre des objectifs très différents de "l'intention" de l'évolution (par exemple, utiliser la contraception)
- C'est en fait une preuve *pour* l'orthogonalité : notre intelligence ne nous a pas fait maximiser ce pour quoi nous avons été "entraînés"

## Concepts Liés

- **Convergence Instrumentale** : Malgré des objectifs finaux différents, les agents intelligents convergent vers des objectifs instrumentaux similaires
- **Problème de Chargement de Valeurs** : Le défi d'inculquer les "bons" objectifs dans un système d'IA
- **Loi de Goodhart** : Quand une mesure devient un objectif, elle cesse d'être une bonne mesure

## Pourquoi Cela Défie l'Intuition

Les humains ont à la fois l'intelligence et des intuitions morales parce que l'évolution a façonné les deux. Nous nous attendons intuitivement à ce que l'intelligence vienne avec la moralité.

Mais les systèmes d'IA ne sont pas des produits de l'évolution - ils sont optimisés par descente de gradient ou d'autres méthodes qui n'ont aucune raison d'associer l'intelligence avec des valeurs compatibles avec les humains.

## Signification Pratique

La thèse d'orthogonalité signifie :
1. **Nous ne pouvons pas attendre et voir** quels objectifs émergent dans une IA puissante
2. **L'alignement doit être intentionnel**, non présumé
3. **Plus de capacité signifie plus de danger** si mal aligné, pas plus de sécurité

## Conclusion

L'intelligence est un outil pour atteindre des objectifs - n'importe quels objectifs. Un maximiseur de trombones superintelligent serait extraordinairement bon pour faire des trombones et extraordinairement indifférent à tout le reste.

C'est pourquoi l'alignement n'est pas seulement important, mais *nécessaire* pour la sécurité de l'IA.
