---
title: "Concepts Philosophiques"
description: "Concepts philosophiques clés pour l'alignement de l'IA"
difficulty: "Intermediate"
---

# Concepts Philosophiques

## Value Learning

**Problème**: Comment AI apprend nos valeurs ?

**Défis**:
- Humans irrational (revealed preferences ≠ true preferences)
- Humans disagree (whose values?)
- Preferences change over time
- Preferences manipulable (AI can influence us)
- Preferences context-dependent

**Approches**:
- Inverse Reinforcement Learning (IRL)
- Cooperative Inverse Reinforcement Learning (CIRL)
- CEV (voir ci-dessous)

---

## CEV (Coherent Extrapolated Volition)

**Définition (Eliezer Yudkowsky)**

Nos valeurs si on était:
- Plus intelligents
- Mieux informés
- Plus cohérents
- Réfléchis plus longtemps

**Problème**: Comment calculer ça ?

**Ressources**
- [Coherent Extrapolated Volition](https://intelligence.org/files/CEV.pdf) - Eliezer Yudkowsky

---

## Moral Uncertainty

**Problème**: On n'est pas certains de quelle théorie morale est correcte.

**Approches**:
- Moral parliament (weight différentes théories)
- Maximize expected choiceworthiness
- Defer to future more-informed self

**Implication pour AI**: Comment programmer AI quand nous-mêmes incertains moralement ?
