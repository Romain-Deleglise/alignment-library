---
title: "Actes Pivots"
description: "Actions suffisamment puissantes pour empêcher une AGI non alignée de détruire le monde"
difficulty: "advanced"
readingTime: "12 min"
tags: ["strategy", "governance", "capability", "risk"]
prerequisites: ["introduction/what-is-alignment", "concepts/intelligence-explosion"]
---

# Actes Pivots

## Définition

Un **acte pivot** est une action qui empêche la création d'une AGI non alignée et la destruction du monde, exécutée en utilisant une IA alignée avant que d'autres acteurs ne développent une AGI non alignée.

Le terme vient de la question stratégique : *"Que faudrait-il faire avec l'IA pour empêcher tout le monde de mourir ?"*

## Le Problème Central

De la "Liste des Léthalités" d'Eliezer Yudkowsky :

> "Nous devons aligner la performance d'une grande tâche, un 'acte pivot' qui empêche d'autres personnes de construire une AGI non alignée qui détruit le monde."

### Pourquoi les Actes Pivots Sont Nécessaires

1. **Acteurs multiples** : Les connaissances et le matériel se répandent
2. **Pression temporelle** : Plus d'acteurs acquièrent la capacité AGI au fil du temps
3. **Un seul échec suffit** : Une seule AGI non alignée détruit le monde
4. **Pas de seconde chance** : Vous ne pouvez pas réessayer après la première AGI non alignée

## Propriétés Clés

### Doit Être Suffisamment Puissant

L'acte doit **empêcher toutes les autres AGI non alignées** d'être créées suffisamment longtemps pour résoudre le problème plus large.

Cela signifie qu'il doit affecter le monde entier, pas seulement votre environnement local.

### Exemples (Pour Illustration)

L'exemple canonique d'Eliezer : **"Brûler tous les GPU"**

- Déployer des nanomachines pour faire fondre toutes les puces GPU dans le monde
- Empêche quiconque d'entraîner une IA dangereuse
- Gagne du temps pour résoudre correctement l'alignement

**Important** : Ceci est intentionnellement choisi comme quelque chose :
- **En dehors de la fenêtre d'Overton** (politiquement inacceptable)
- **Démontrant le niveau de puissance requis**
- **Pas réellement la stratégie optimale**

Comme Eliezer le note : "Ce n'est pas ce que je pense que vous voudriez réellement faire... c'est juste une légère surestimation du niveau de puissance approximatif."

### Doit Utiliser une IA Alignée

L'acte pivot nécessite des capacités au-delà de ce que les humains peuvent faire seuls (sinon nous pourrions simplement le faire maintenant), mais doit être exécuté par une IA que nous pouvons aligner en toute sécurité.

Cela crée un dilemme : **l'IA doit être puissante mais sûre.**

## Le Problème "Pas d'Actes Pivots Faibles"

De la Liste des Léthalités :

> "Il n'y a aucune raison pour que cela existe. Il n'y a pas de raison élaborée et intelligente pour laquelle cela existerait mais que personne ne peut voir."

### Pourquoi Aucun Acte Pivot Faible N'Existe

1. **Nécessite un effet global** : Doit empêcher la fabrication/utilisation de GPU dans le monde entier
2. **Acteurs opposés** : Certains acteurs ne veulent pas être arrêtés
3. **Différentiel de puissance** : Doit surmonter la résistance d'acteurs au niveau étatique
4. **Pas de capacité actuelle** : Si les humains pouvaient le faire maintenant, nous le ferions

### La Fantaisie d'"Améliorer l'Épistémologie"

De nombreuses propositions suggèrent des actes faibles comme :
- "Utiliser GPT-5 pour améliorer le discours public"
- "Créer de meilleurs outils de détection de mensonges"
- "Aider les gens à prendre de meilleures décisions"

**Ceux-ci ne fonctionnent pas parce que :**
- Ils n'empêchent pas Facebook AI Research de construire une AGI non alignée
- Ils n'arrêtent pas les acteurs étatiques déterminés
- Ils ne répondent pas au surplomb matériel
- Ils opèrent sur des échelles de temps (années) plus longues que le temps restant (mois)

## Difficulté d'Alignement

### Le Test de l'Acte Pivot

Un bon test d'une proposition d'alignement :

> "Comment pourriez-vous utiliser cela pour aligner un système que vous pourriez utiliser pour arrêter tous les GPU dans le monde ?"

De nombreuses idées d'alignement échouent à ce test parce que :
- Elles ne fonctionnent que sur des systèmes faibles et sûrs
- Elles nécessitent un débogage itératif (pas de temps pour cela)
- Elles supposent que vous pouvez simplement "ne pas lui donner de capacités dangereuses"
- Elles ne répondent pas à la conscience stratégique et à la tromperie

### Exigences

Pour exécuter un acte pivot, votre IA doit :
1. **Être suffisamment capable** pour affecter le monde entier
2. **Être suffisamment alignée** pour ne pas tuer tout le monde à la place
3. **Fonctionner du premier coup** (pas d'itération à des niveaux de capacité dangereux)
4. **Opérer dans des domaines dangereux** (nanotechnologie, planification stratégique, etc.)

## Le Dilemme Temporel

### Trop Tôt
- Votre IA n'est pas assez capable pour exécuter l'acte pivot
- Vous ne pouvez pas empêcher les autres acteurs de procéder

### Trop Tard
- Un autre acteur a déjà construit une AGI non alignée
- Le monde est déjà condamné

### La Fenêtre Étroite

La fenêtre viable est :
- **Après** que vous puissiez aligner une IA suffisamment puissante pour la tâche
- **Avant** que quelqu'un d'autre construise une AGI non alignée

Cette fenêtre peut être :
- Quelques mois
- Quelques semaines
- Inexistante (si l'alignement est plus difficile que les capacités)

## Objections Courantes

### "Ça semble autoritaire et dangereux !"

Oui. C'est le point.

L'alternative est que **tout le monde meurt**. Les actes pivots ne sont pas censés être plaisants ou politiquement acceptables - ils sont censés être suffisants pour prévenir l'extinction.

La question n'est pas "Est-ce gentil ?" mais "L'alternative est-elle pire ?"

### "Ne pouvons-nous pas simplement coordonner internationalement ?"

Statut actuel de la coordination internationale sur l'IA :
- Pas de traités contraignants
- Plusieurs projets concurrents
- Fortes incitations économiques à faire défection
- Pas de mécanisme d'application
- Plusieurs acteurs rejettent ouvertement les préoccupations de sécurité

Même si les principaux laboratoires se coordonnent, qu'en est-il :
- Des projets open-source ?
- Des acteurs étatiques (Chine, Russie, etc.) ?
- Des acteurs futurs avec plus de matériel ?

### "Pourquoi ne pas simplement convaincre tout le monde d'arrêter ?"

Cela nécessite :
- **Coordination mondiale parfaite** (sans précédent)
- **Arrêt permanent** du progrès matériel et logiciel
- **Application** de l'arrêt à l'échelle mondiale
- **Aucun défecteur** jamais

Cela n'a jamais fonctionné pour aucune technologie dans l'histoire humaine.

## Cadrages Alternatifs

Certains préfèrent une terminologie différente :
- **Stabilisation** : Rendre le monde sûr face au risque AGI
- **Stratégies défensives** : Empêcher le déploiement d'AGI nuisible
- **Jalons d'alarme incendie** : Signaux nécessitant une réponse immédiate

Le concept central reste : **quelque chose doit être fait qui soit suffisamment puissant pour fonctionner réellement.**

## Implications pour la Recherche

### Cible d'Alignement

Votre approche d'alignement doit s'adapter à des systèmes suffisamment puissants pour exécuter des actes pivots.

"Sûr mais inutile" n'est pas suffisant - cela signifie simplement que quelqu'un d'autre détruit le monde.

### Préoccupations de Capacité

Décrire les actes pivots révèle :
- L'ampleur réelle de la capacité nécessaire
- Pourquoi "faible et sûr" n'est pas une stratégie viable
- Pourquoi le développement itératif peut ne pas fonctionner

### Conscience Stratégique

Une IA capable d'actes pivots doit être :
- Stratégiquement consciente (comprend les enjeux)
- Mais pas adversaire (ne se retourne pas contre les opérateurs)
- Cette combinaison est très difficile à atteindre

## La Vérité Inconfortable

La plupart des gens, en entendant parler d'actes pivots, éprouvent une combinaison de :
- **Déni** : "Ce ne peut pas être nécessaire"
- **Colère** : "C'est contraire à l'éthique/autoritaire"
- **Négociation** : "Ne pouvons-nous pas trouver une alternative plus faible ?"
- **Dépression** : "Nous sommes condamnés de toute façon"

Ce sont des réactions compréhensibles à une réalité stratégique inconfortable :

**Soit quelqu'un exécute un acte pivot avec une IA alignée, soit quelqu'un construit une AGI non alignée et tout le monde meurt.**

Il n'y a pas de troisième option confortable où nous acceptons tous poliment d'arrêter et le problème disparaît.

## Conclusion

Les actes pivots représentent la condition de succès minimale viable pour la sécurité de l'IA :

> Faire quelque chose d'assez puissant pour empêcher une AGI non alignée, en utilisant une IA qui est suffisamment alignée pour ne pas tuer tout le monde elle-même.

Le concept force une réflexion claire sur :
- Combien de puissance est réellement nécessaire
- Ce que "l'alignement" doit accomplir
- Pourquoi les demi-mesures faibles ne fonctionneront pas
- Les véritables enjeux du problème

Que vous aimiez ou non le cadrage, la réalité stratégique sous-jacente demeure :

**Nous devons résoudre l'alignement pour des systèmes suffisamment puissants pour avoir de l'importance, avant que quiconque construise des systèmes non alignés aussi puissants.**

Comme Eliezer l'écrit : *"Si vous ne pouvez pas résoudre le problème maintenant, alors vous recourez à un système cognitif qui peut faire des choses que vous ne pourriez pas comprendre comment faire vous-même."*

Et ce système doit être aligné.
