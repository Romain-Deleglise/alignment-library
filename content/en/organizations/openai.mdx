---
title: "OpenAI Safety Team"
description: "AI safety research at OpenAI"
difficulty: "beginner"
readingTime: "15 min"
tags: ["organizations","openai","industry"]
prerequisites: []
---

# OpenAI Safety Team

## Mission

Ensure AGI benefits all of humanity.

## Founded

2015

## Key Safety People

- Jan Leike (alignment team, left 2024)
- John Schulman (alignment, left 2024)
- Ilya Sutskever (chief scientist, left 2024)

## Safety Research

- RLHF / InstructGPT
- Debate
- Process supervision
- Rule-based rewards

## Products

- GPT-3, GPT-4
- ChatGPT
- DALL-E

## Key Publications

- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [Learning to Summarize with Human Feedback](https://arxiv.org/abs/2009.01325)
- [AI Safety via Debate](https://arxiv.org/abs/1805.00899)

## Concerns

- Rapid capability advancement
- Key safety researchers leaving
- Dissolved superalignment team (2024)
- Commercial pressure vs safety

## Website

[openai.com/safety](https://openai.com/safety)
