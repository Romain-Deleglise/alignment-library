---
title: "Key Researchers in AI Alignment"
description: "Leading figures in AI alignment research"
difficulty: "beginner"
readingTime: "20 min"
tags: ["organizations","researchers","people"]
prerequisites: []
---

# Key Researchers in AI Alignment

## Pioneers

### Eliezer Yudkowsky
- Founder of MIRI
- Articulated AI risk early (2000s)
- Wrote "AGI Ruin: A List of Lethalities"
- Position: ~99% P(doom)

### Stuart Russell
- Berkeley professor
- Author "Human Compatible"
- Advocacy for AI safety in academia
- Promotes value learning

### Nick Bostrom
- Oxford philosopher
- Wrote "Superintelligence" (2014)
- Director of Future of Humanity Institute
- Brought AI risk to mainstream

## Technical Researchers

### Paul Christiano
- Former OpenAI, now ARC
- Developed IDA (Iterated Amplification)
- ELK problem formulation
- Position: ~50-70% P(doom)

### Nate Soares
- Executive Director of MIRI
- Agent foundations research
- Position: ~90%+ P(doom)

### Ajeya Cotra
- AI forecasting
- Biological Anchors framework
- Timeline estimates

## Current Leaders

### Dario Amodei (Anthropic)
- Constitutional AI
- Former OpenAI VP of Research

### Chris Olah (Anthropic)
- Mechanistic interpretability
- Neural network visualization

### Jan Leike (ex-OpenAI, now Anthropic)
- Alignment team lead
- RLHF research

## Where to Follow

- [Alignment Forum](https://alignmentforum.org)
- [LessWrong](https://lesswrong.com)
- Twitter/X
