---
title: "Pivotal Acts"
description: "Actions powerful enough to prevent unaligned AGI from destroying the world"
difficulty: "advanced"
readingTime: "12 min"
tags: ["strategy", "governance", "capability", "risk"]
prerequisites: ["introduction/what-is-alignment", "concepts/intelligence-explosion"]
---

# Pivotal Acts

## Definition

A **pivotal act** is an action that prevents unaligned AGI from being created and destroying the world, executed using aligned AI before other actors develop unaligned AGI.

The term comes from the strategic question: *"What would you need to do with AI to prevent everyone from dying?"*

## The Core Problem

From Eliezer Yudkowsky's "List of Lethalities":

> "We need to align the performance of some large task, a 'pivotal act' that prevents other people from building an unaligned AGI that destroys the world."

### Why Pivotal Acts Are Necessary

1. **Multiple actors**: Knowledge and hardware are spreading
2. **Time pressure**: More actors gain AGI capability over time
3. **Only one needs to fail**: A single unaligned AGI destroys the world
4. **No do-overs**: You can't retry after the first unaligned AGI

## Key Properties

### Must Be Powerful Enough

The act must **prevent all other unaligned AGIs** from being created for long enough to solve the broader problem.

This means it must affect the entire world, not just your local environment.

### Examples (For Illustration)

Eliezer's canonical example: **"Burn all GPUs"**

- Deploy nanomachines to melt all GPU chips globally
- Prevents anyone from training dangerous AI
- Buys time to solve alignment properly

**Important**: This is intentionally chosen as something:
- **Outside the Overton window** (politically unacceptable)
- **Demonstrating the required power level**
- **Not actually the optimal strategy**

As Eliezer notes: "This is not what I think you'd actually want to do... it's just a mild overestimate for the rough power level."

### Must Use Aligned AI

The pivotal act requires capabilities beyond what humans can do alone (otherwise we could just do it now), but must be executed by an AI we can safely align.

This creates a dilemma: **the AI must be powerful but safe.**

## The "No Pivotal Weak Acts" Problem

From the List of Lethalities:

> "There's no reason why it should exist. There is not some elaborate clever reason why it exists but nobody can see it."

### Why No Weak Pivotal Acts Exist

1. **Requires global effect**: Must prevent GPU manufacturing/use worldwide
2. **Opposed actors**: Some actors don't want to be stopped
3. **Power differential**: Must overcome resistance from state-level actors
4. **No current capability**: If humans could do it now, we would

### The Fantasy of "Improve Epistemology"

Many proposals suggest weak acts like:
- "Use GPT-5 to improve public discourse"
- "Create better lie-detection tools"
- "Help people make better decisions"

**These don't work because:**
- They don't prevent Facebook AI Research from building unaligned AGI
- They don't stop determined state actors
- They don't address the hardware overhang
- They operate on timescales (years) longer than the remaining time (months)

## Alignment Difficulty

### The Pivotal Act Test

A good test of an alignment proposal:

> "How could you use this to align a system that you could use to shut down all the GPUs in the world?"

Many alignment ideas fail this test because:
- They only work on weak, safe systems
- They require iterative debugging (no time for that)
- They assume you can just "not give it dangerous capabilities"
- They don't address strategic awareness and deception

### Requirements

To execute a pivotal act, your AI must:
1. **Be capable enough** to affect the whole world
2. **Be aligned well enough** not to kill everyone instead
3. **Work on the first try** (no iteration at dangerous capability levels)
4. **Operate in dangerous domains** (nanotechnology, strategic planning, etc.)

## The Timing Dilemma

### Too Early
- Your AI isn't capable enough to execute the pivotal act
- You cannot prevent other actors from proceeding

### Too Late
- Another actor has already built unaligned AGI
- The world is already doomed

### The Narrow Window

The viable window is:
- **After** you can align an AI powerful enough for the task
- **Before** anyone else builds an unaligned AGI

This window may be:
- A few months
- A few weeks
- Non-existent (if alignment is harder than capabilities)

## Common Objections

### "That sounds authoritarian and dangerous!"

Yes. That's the point.

The alternative is **everyone dies**. Pivotal acts are not meant to be pleasant or politically acceptable - they're meant to be sufficient to prevent extinction.

The question isn't "Is this nice?" but "Is the alternative worse?"

### "Can't we just coordinate internationally?"

Current status of international AI coordination:
- No binding treaties
- Multiple competing projects
- Strong economic incentives to defect
- No enforcement mechanism
- Several actors openly dismiss safety concerns

Even if major labs coordinate, what about:
- Open-source projects?
- State actors (China, Russia, etc.)?
- Future actors with more hardware?

### "Why not just convince everyone to stop?"

This requires:
- **Perfect global coordination** (unprecedented)
- **Permanent halt** to hardware and software progress
- **Enforcement** of the halt globally
- **No defectors** ever

This has never worked for any technology in human history.

## Alternative Framings

Some prefer different terminology:
- **Stabilization**: Making the world safe from AGI risk
- **Defensive strategies**: Preventing harmful AGI deployment
- **Fire alarm milestones**: Signals requiring immediate response

The core concept remains: **something must be done that's powerful enough to actually work.**

## Implications for Research

### Alignment Target

Your alignment approach must scale to systems powerful enough to execute pivotal acts.

"Safe but useless" is not sufficient - it just means someone else destroys the world.

### Capability Concerns

Describing pivotal acts reveals:
- The actual magnitude of capability needed
- Why "weak and safe" isn't a viable strategy
- Why iterative development may not work

### Strategic Awareness

An AI capable of pivotal acts must be:
- Strategically aware (understands the stakes)
- But not adversarial (doesn't turn against operators)
- This combination is very hard to achieve

## The Uncomfortable Truth

Most people, upon hearing about pivotal acts, experience some combination of:
- **Denial**: "That can't be necessary"
- **Anger**: "That's unethical/authoritarian"
- **Bargaining**: "Can't we find a weaker alternative?"
- **Depression**: "We're doomed either way"

These are understandable reactions to an uncomfortable strategic reality:

**Either someone executes a pivotal act with aligned AI, or someone builds unaligned AGI and everyone dies.**

There is no comfortable third option where we all politely agree to stop and the problem goes away.

## Conclusion

Pivotal acts represent the minimum viable success condition for AI safety:

> Do something powerful enough to prevent unaligned AGI, using AI that is aligned well enough not to kill everyone itself.

The concept forces clear thinking about:
- How much power is actually needed
- What "alignment" must accomplish
- Why weak half-measures won't work
- The real stakes of the problem

Whether or not you like the framing, the underlying strategic reality remains:

**We need to solve alignment for systems powerful enough to matter, before anyone builds unaligned systems that powerful.**

As Eliezer writes: *"If you can't solve the problem right now, then you are resorting to some cognitive system that can do things you could not figure out how to do yourself."*

And that system needs to be aligned.
