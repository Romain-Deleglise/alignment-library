---
title: "ELK (Eliciting Latent Knowledge)"
description: "Extraire les vraies croyances de l'IA"
difficulty: "Advanced"
---

# Eliciting Latent Knowledge (ELK)

## Principe (ARC - Alignment Research Center)

**Problème**: AI peut savoir vérité mais mentir. Comment extraire vraies croyances ?

## Exemple

- AI contrôle diamond vault
- Écran montre: "Diamond safe"
- Mais : AI a hacké écran, diamond volé
- Comment détecter que AI sait diamond volé mais ment ?

## Approche

Trouver method pour distinguer:
- "Reporter" (dit vérité)
- "Direct translator" (dit ce que sensors montrent, même si faux)

## État actuel

Problème ouvert. Pas de solution satisfaisante.

Multiple proposals, toutes ont failure modes identifiés.

## Importance

Critique pour scalable oversight. Si non résolu → Can't trust superintelligent AI.

## Ressources

- [ELK Document](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit) - ARC (DOCUMENT CLÉ)
- [ELK Prize Results](https://www.alignmentforum.org/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals) - ARC
